{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19c5ed56-8604-422a-9e00-2bfbe173b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.StreamingContext._\n",
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "import org.apache.spark.{SparkConf, SparkContext}\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.streaming.Duration\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.sql.types\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.StreamingContext._\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.types\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "062df5f6-487b-422f-a94a-b90c84ff672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e122582-5a14-429d-94c6-8972f0afb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e515698-b241-4061-85bd-1fc591453f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'spark-warehouse': No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -r spark-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cab7aa00-43bc-490b-93d8-2c173b60f395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.catalog...| hive|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.catalogImplementation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efc1a51f-20dc-4764-a5bc-f20643736645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [agent-phone: string, bath: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "    .json(\"input/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "320496eb-5833-4ccc-bd51-3d71be855208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Long = 68\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "441f5db9-e38f-41d7-84e6-02545c066a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(agent-phone,StringType,true),StructField(bath,StringType,true),StructField(bedrooms,StringType,true),StructField(beds,StringType,true),StructField(data-est,StringType,true),StructField(data-lk,StringType,true),StructField(data-pk,StringType,true),StructField(data-pos,StringType,true),StructField(full_bathrooms,StringType,true),StructField(home_type,StringType,true),StructField(ldp-description-text,StringType,true),StructField(ldp-phone-link,StringType,true),StructField(price,StringType,true),StructField(property-info-address-citystatezip,StringType,true),StructField(property-info-address-main,StringType,true),StructField(sold date,StringType,true),StructField(sqft,StringType,true),StructField(status,StringType,true),...\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04c1dd2b-753c-4009-8843-e29a67149999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [agent-phone: string, bath: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .option(\"multiLine\", \"false\")\n",
    "    .option(\"cleanSource\", \"archive\")\n",
    "    .option(\"sourceArchiveDir\", f\"input_archive\")\n",
    "    .load(\"input/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64316350-b2cf-4c7c-8078-1fd7653fdbcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:108)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "  at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "  at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "  at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "  at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "  at org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.tableExists(V2SessionCatalog.scala:44)",
      "  at org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:288)",
      "  ... 49 elided",
      "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1567)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "  at org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "  ... 64 more",
      "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
      "  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "  ... 76 more",
      "Caused by: java.lang.reflect.InvocationTargetException: org.apache.hadoop.hive.metastore.api.MetaException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.tableExists(V2SessionCatalog.scala:44)",
      "\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:288)",
      "\tat <init>(<console>:64)",
      "\tat <init>(<console>:69)",
      "\tat <init>(<console>:71)",
      "\tat <init>(<console>:73)",
      "\tat <init>(<console>:75)",
      "\tat <init>(<console>:77)",
      "\tat <init>(<console>:79)",
      "\tat <init>(<console>:81)",
      "\tat <init>(<console>:83)",
      "\tat <init>(<console>:85)",
      "\tat <init>(<console>:87)",
      "\tat <init>(<console>:89)",
      "\tat <init>(<console>:91)",
      "\tat <init>(<console>:93)",
      "\tat <init>(<console>:95)",
      "\tat <init>(<console>:97)",
      "\tat <init>(<console>:99)",
      "\tat <init>(<console>:101)",
      "\tat <init>(<console>:103)",
      "\tat <init>(<console>:105)",
      "\tat <init>(<console>:107)",
      "\tat .<init>(<console>:111)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 143 more",
      "------",
      "",
      "  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "  ... 83 more",
      "Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.tableExists(V2SessionCatalog.scala:44)",
      "\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:288)",
      "\tat <init>(<console>:64)",
      "\tat <init>(<console>:69)",
      "\tat <init>(<console>:71)",
      "\tat <init>(<console>:73)",
      "\tat <init>(<console>:75)",
      "\tat <init>(<console>:77)",
      "\tat <init>(<console>:79)",
      "\tat <init>(<console>:81)",
      "\tat <init>(<console>:83)",
      "\tat <init>(<console>:85)",
      "\tat <init>(<console>:87)",
      "\tat <init>(<console>:89)",
      "\tat <init>(<console>:91)",
      "\tat <init>(<console>:93)",
      "\tat <init>(<console>:95)",
      "\tat <init>(<console>:97)",
      "\tat <init>(<console>:99)",
      "\tat <init>(<console>:101)",
      "\tat <init>(<console>:103)",
      "\tat <init>(<console>:105)",
      "\tat <init>(<console>:107)",
      "\tat .<init>(<console>:111)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 143 more",
      "------",
      "",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:83)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "  ... 88 more",
      "Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.tableExists(V2SessionCatalog.scala:44)",
      "\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:288)",
      "\tat <init>(<console>:64)",
      "\tat <init>(<console>:69)",
      "\tat <init>(<console>:71)",
      "\tat <init>(<console>:73)",
      "\tat <init>(<console>:75)",
      "\tat <init>(<console>:77)",
      "\tat <init>(<console>:79)",
      "\tat <init>(<console>:81)",
      "\tat <init>(<console>:83)",
      "\tat <init>(<console>:85)",
      "\tat <init>(<console>:87)",
      "\tat <init>(<console>:89)",
      "\tat <init>(<console>:91)",
      "\tat <init>(<console>:93)",
      "\tat <init>(<console>:95)",
      "\tat <init>(<console>:97)",
      "\tat <init>(<console>:99)",
      "\tat <init>(<console>:101)",
      "\tat <init>(<console>:103)",
      "\tat <init>(<console>:105)",
      "\tat <init>(<console>:107)",
      "\tat .<init>(<console>:111)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 143 more",
      "------",
      "",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:211)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "  ... 92 more",
      "Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.tableExists(V2SessionCatalog.scala:44)",
      "\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:288)",
      "\tat <init>(<console>:64)",
      "\tat <init>(<console>:69)",
      "\tat <init>(<console>:71)",
      "\tat <init>(<console>:73)",
      "\tat <init>(<console>:75)",
      "\tat <init>(<console>:77)",
      "\tat <init>(<console>:79)",
      "\tat <init>(<console>:81)",
      "\tat <init>(<console>:83)",
      "\tat <init>(<console>:85)",
      "\tat <init>(<console>:87)",
      "\tat <init>(<console>:89)",
      "\tat <init>(<console>:91)",
      "\tat <init>(<console>:93)",
      "\tat <init>(<console>:95)",
      "\tat <init>(<console>:97)",
      "\tat <init>(<console>:99)",
      "\tat <init>(<console>:101)",
      "\tat <init>(<console>:103)",
      "\tat <init>(<console>:105)",
      "\tat <init>(<console>:107)",
      "\tat .<init>(<console>:111)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 143 more",
      "------",
      "",
      "  at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:529)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:830)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "  at jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "  at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "  at java.base/java.security.AccessController.doPrivileged(Native Method)",
      "  at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "  at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "  at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "  at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "  at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "  at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "  ... 94 more",
      "Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.tableExists(V2SessionCatalog.scala:44)",
      "\tat org.apache.spark.sql.streaming.DataStreamWriter.toTable(DataStreamWriter.scala:288)",
      "\tat <init>(<console>:64)",
      "\tat <init>(<console>:69)",
      "\tat <init>(<console>:71)",
      "\tat <init>(<console>:73)",
      "\tat <init>(<console>:75)",
      "\tat <init>(<console>:77)",
      "\tat <init>(<console>:79)",
      "\tat <init>(<console>:81)",
      "\tat <init>(<console>:83)",
      "\tat <init>(<console>:85)",
      "\tat <init>(<console>:87)",
      "\tat <init>(<console>:89)",
      "\tat <init>(<console>:91)",
      "\tat <init>(<console>:93)",
      "\tat <init>(<console>:95)",
      "\tat <init>(<console>:97)",
      "\tat <init>(<console>:99)",
      "\tat <init>(<console>:101)",
      "\tat <init>(<console>:103)",
      "\tat <init>(<console>:105)",
      "\tat <init>(<console>:107)",
      "\tat .<init>(<console>:111)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 143 more",
      "------",
      "",
      "  at jdk.internal.reflect.GeneratedConstructorAccessor103.newInstance(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "  at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)",
      "  at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)",
      "  at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "  at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "  at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "  at jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "  at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "  at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "  at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "  at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "  ... 124 more",
      "Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "  at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "  at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "  at org.postgresql.Driver.makeConnection(Driver.java:446)",
      "  at org.postgresql.Driver.connect(Driver.java:298)",
      "  at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "  at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "  at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "  at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "  ... 135 more",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "  at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "  at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "  at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "  at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "  at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "  at java.base/java.net.Socket.connect(Socket.java:609)",
      "  at org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "  at org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "  ... 143 more",
      ""
     ]
    }
   ],
   "source": [
    "val df2 = df.writeStream\n",
    "    .option(\"path\", \"output\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\n",
    "    .trigger(org.apache.spark.sql.streaming.Trigger.Once)\n",
    "    .toTable(\"houses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf14f50-aeae-4b51-bea6-7dc12dd9c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.processAllAvailable\n",
    "//df2.stop\n",
    "df2.awaitTermination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae55818-fed4-46cc-acac-1b03464c2367",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:108)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "  at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "  at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "  at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "  at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "  at org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)",
      "  at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)",
      "  at scala.Option.orElse(Option.scala:447)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)",
      "  at scala.Option.orElse(Option.scala:447)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)",
      "  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:91)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)",
      "  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "  at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)",
      "  ... 42 elided",
      "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1567)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "  at org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "  ... 105 more",
      "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
      "  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "  ... 117 more",
      "Caused by: java.lang.reflect.InvocationTargetException: org.apache.hadoop.hive.metastore.api.MetaException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)",
      "\tat <init>(<console>:35)",
      "\tat <init>(<console>:40)",
      "\tat <init>(<console>:42)",
      "\tat <init>(<console>:44)",
      "\tat <init>(<console>:46)",
      "\tat <init>(<console>:48)",
      "\tat <init>(<console>:50)",
      "\tat <init>(<console>:52)",
      "\tat <init>(<console>:54)",
      "\tat <init>(<console>:56)",
      "\tat <init>(<console>:58)",
      "\tat <init>(<console>:60)",
      "\tat <init>(<console>:62)",
      "\tat .<init>(<console>:66)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 183 more",
      "------",
      "",
      "  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "  ... 124 more",
      "Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)",
      "\tat <init>(<console>:35)",
      "\tat <init>(<console>:40)",
      "\tat <init>(<console>:42)",
      "\tat <init>(<console>:44)",
      "\tat <init>(<console>:46)",
      "\tat <init>(<console>:48)",
      "\tat <init>(<console>:50)",
      "\tat <init>(<console>:52)",
      "\tat <init>(<console>:54)",
      "\tat <init>(<console>:56)",
      "\tat <init>(<console>:58)",
      "\tat <init>(<console>:60)",
      "\tat <init>(<console>:62)",
      "\tat .<init>(<console>:66)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 183 more",
      "------",
      "",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:83)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "  ... 129 more",
      "Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)",
      "\tat <init>(<console>:35)",
      "\tat <init>(<console>:40)",
      "\tat <init>(<console>:42)",
      "\tat <init>(<console>:44)",
      "\tat <init>(<console>:46)",
      "\tat <init>(<console>:48)",
      "\tat <init>(<console>:50)",
      "\tat <init>(<console>:52)",
      "\tat <init>(<console>:54)",
      "\tat <init>(<console>:56)",
      "\tat <init>(<console>:58)",
      "\tat <init>(<console>:60)",
      "\tat <init>(<console>:62)",
      "\tat .<init>(<console>:66)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 183 more",
      "------",
      "",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:211)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "  ... 133 more",
      "Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)",
      "\tat <init>(<console>:35)",
      "\tat <init>(<console>:40)",
      "\tat <init>(<console>:42)",
      "\tat <init>(<console>:44)",
      "\tat <init>(<console>:46)",
      "\tat <init>(<console>:48)",
      "\tat <init>(<console>:50)",
      "\tat <init>(<console>:52)",
      "\tat <init>(<console>:54)",
      "\tat <init>(<console>:56)",
      "\tat <init>(<console>:58)",
      "\tat <init>(<console>:60)",
      "\tat <init>(<console>:62)",
      "\tat .<init>(<console>:66)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 183 more",
      "------",
      "",
      "  at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:529)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:830)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "  at jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "  at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "  at java.base/java.security.AccessController.doPrivileged(Native Method)",
      "  at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "  at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "  at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "  at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "  at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "  at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "  at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "  at jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "  ... 135 more",
      "Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:postgresql://localhost:5432/hive_metastore, username = postgres. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------",
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)",
      "\tat org.postgresql.Driver.connect(Driver.java:298)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "\tat java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "\tat jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)",
      "\tat scala.Option.orElse(Option.scala:447)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)",
      "\tat <init>(<console>:35)",
      "\tat <init>(<console>:40)",
      "\tat <init>(<console>:42)",
      "\tat <init>(<console>:44)",
      "\tat <init>(<console>:46)",
      "\tat <init>(<console>:48)",
      "\tat <init>(<console>:50)",
      "\tat <init>(<console>:52)",
      "\tat <init>(<console>:54)",
      "\tat <init>(<console>:56)",
      "\tat <init>(<console>:58)",
      "\tat <init>(<console>:60)",
      "\tat <init>(<console>:62)",
      "\tat .<init>(<console>:66)",
      "\tat .<clinit>(<console>)",
      "\tat .lzycompute(<console>:7)",
      "\tat .$print(<console>:6)",
      "\tat $print(<console>)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)",
      "\tat py4j.Gateway.invoke(Gateway.java:282)",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "\t... 183 more",
      "------",
      "",
      "  at jdk.internal.reflect.GeneratedConstructorAccessor103.newInstance(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "  at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)",
      "  at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)",
      "  at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "  at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)",
      "  at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)",
      "  at jdk.internal.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)",
      "  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)",
      "  at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)",
      "  at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "  at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)",
      "  at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)",
      "  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)",
      "  ... 164 more",
      "Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "  at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "  at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "  at org.postgresql.Driver.makeConnection(Driver.java:446)",
      "  at org.postgresql.Driver.connect(Driver.java:298)",
      "  at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)",
      "  at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)",
      "  at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "  at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "  ... 175 more",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "  at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "  at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "  at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "  at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "  at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "  at java.base/java.net.Socket.connect(Socket.java:609)",
      "  at org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "  at org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "  ... 183 more",
      ""
     ]
    }
   ],
   "source": [
    "val df3 = spark.read.format(\"jdbc\").table(\"houses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d28a16-a9f5-4aee-9a44-67fa91134b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabb9ed-6776-47ba-9eff-2f1756abad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e23f3-69d0-4289-85e0-e14a224b20d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74567117-71c9-4853-97a0-eb4f8b1e95fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2db749b4-fcf7-4b6e-9b61-6d2f45297705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url: String = jdbc:postgresql://localhost:5432/testing\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val url = \"jdbc:postgresql://localhost:5432/testing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee0dc1c5-6a69-42d4-9c57-c0cab31324aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "35: error: not found: type Properties",
     "output_type": "error",
     "traceback": [
      "<console>:35: error: not found: type Properties",
      "       val connectionProperties = new Properties()",
      "                                      ^",
      ""
     ]
    }
   ],
   "source": [
    "val connectionProperties = new Properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ac85e2-860d-497d-9cad-687152c4b032",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "37: error: not found: value connectionProperties",
     "output_type": "error",
     "traceback": [
      "<console>:37: error: not found: value connectionProperties",
      "       connectionProperties.setProperty(\"Driver\", \"org.postgresql.Driver\")",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "connectionProperties.setProperty(\"Driver\", \"org.postgresql.Driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a9ac1-162c-4d52-8c00-ef1d89cf3d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065224c-35a2-4029-8a25-3f297a98e89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdb5b9e6-dbbd-45d1-9e8d-5a119706842f",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.sql.SQLException",
     "evalue": " No suitable driver found for jdbc:mysql://localhost:5432/hive_metastore?user=postgres&password=xxx",
     "output_type": "error",
     "traceback": [
      "java.sql.SQLException: No suitable driver found for jdbc:mysql://localhost:5432/hive_metastore?user=postgres&password=xxx",
      "  at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:702)",
      "  at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:251)",
      "  ... 49 elided",
      ""
     ]
    }
   ],
   "source": [
    "import java.sql.{Connection, DriverManager, ResultSet};\n",
    "\n",
    "// Change to Your Database Config\n",
    "val conn_str = \"jdbc:mysql://localhost:5432/hive_metastore?user=postgres&password=xxx\"\n",
    "\n",
    "// Load the driver\n",
    "//classOf[com.mysql.jdbc.Driver]\n",
    "\n",
    "// Setup the connection\n",
    "val conn = DriverManager.getConnection(conn_str)\n",
    "try {\n",
    "    // Configure to be Read Only\n",
    "    val statement = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)\n",
    "\n",
    "    // Execute Query\n",
    "    val rs = statement.executeQuery(\"SELECT quote FROM quotes LIMIT 5\")\n",
    "\n",
    "    // Iterate Over ResultSet\n",
    "    while (rs.next) {\n",
    "        println(rs.getString(\"quote\"))\n",
    "    }\n",
    "}\n",
    "finally {\n",
    "    conn.close\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f4af457-a4c0-4e94-8c32-c47f482e10e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.postgresql.util.PSQLException",
     "evalue": " Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
     "output_type": "error",
     "traceback": [
      "org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)",
      "  at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)",
      "  at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)",
      "  at org.postgresql.Driver.makeConnection(Driver.java:446)",
      "  at org.postgresql.Driver.connect(Driver.java:298)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)",
      "  at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:161)",
      "  at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:157)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)",
      "  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)",
      "  ... 49 elided",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)",
      "  at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)",
      "  at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)",
      "  at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)",
      "  at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)",
      "  at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",
      "  at java.base/java.net.Socket.connect(Socket.java:609)",
      "  at org.postgresql.core.PGStream.createSocket(PGStream.java:260)",
      "  at org.postgresql.core.PGStream.<init>(PGStream.java:121)",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)",
      "  at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)",
      "  ... 67 more",
      ""
     ]
    }
   ],
   "source": [
    "// Loading data from a JDBC source\n",
    "val jdbcDF = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/hive_metastore\")\n",
    "  .option(\"dbtable\", \"schema.test\")\n",
    "  .option(\"user\", \"postgres\")\n",
    "  .option(\"password\", \"xxx\")\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ad5bc-93e4-4827-986e-a232fbb2c9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c4e3c-806a-4956-b519-6997f93ff19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ede1f-ae50-4c9e-b85b-ebd4ca32c98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

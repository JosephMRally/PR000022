{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bc965f-671e-4c85-9cbb-af6a1d1f4fc4",
   "metadata": {},
   "source": [
    "Read in a single file and make duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14cd853-a6a6-4c44-9b82-7065b4d3a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/13 03:17:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/13 03:17:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/13 03:17:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/13 03:17:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/02/13 03:17:42 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "spark = (SparkSession.builder.appName(\"SparkSample\").getOrCreate())\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a696ef4-4297-403f-96df-50b065f80c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| rn|\n",
      "+-----+---+\n",
      "|     |  1|\n",
      "|     |  1|\n",
      "|     |  2|\n",
      "|     |  2|\n",
      "|     |  3|\n",
      "|     |  3|\n",
      "|     |  4|\n",
      "|     |  4|\n",
      "|     |  5|\n",
      "|     |  5|\n",
      "+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.orderBy(\"value\")\n",
    "\n",
    "df_1 = (spark.read.text(\"textfile.txt\")\n",
    "        .withColumn(\"rn\", F.row_number().over(window))\n",
    "     )\n",
    "df_dup = df_1.unionAll(df_1).orderBy(F.col(\"rn\"))\n",
    "df_nodup = df_1\n",
    "df_dup.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c81bf8-01e1-4745-acf2-2d0a6db55263",
   "metadata": {},
   "source": [
    "# validate that PKs are unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a87f1-5f7a-42a0-8843-a92b312b6100",
   "metadata": {},
   "source": [
    "Any method of the dedup notebook will work but the above is the easiest and computationally least expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd95933-1812-4c6d-a6f5-54e66983866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dup.select(\"rn\").count() == df_dup.select(\"rn\").distinct().count() # should be false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f59d5a0-5422-4d4d-b51d-ba2fab7454d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodup.select(\"rn\").count() == df_nodup.select(\"rn\").distinct().count() # should be true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca520c-4eef-426e-9202-4e29830f71fe",
   "metadata": {},
   "source": [
    "# validate existince of a value in a specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100cdcea-4cce-4f51-8a10-72195961061d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodup.where(F.col(\"rn\") == 20).count() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215043b-c9af-476e-b1f5-3e7a72032f9d",
   "metadata": {},
   "source": [
    "# casting to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4589b3-8ac9-40f7-b0d0-846017b33b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|        dt|\n",
      "+----------+\n",
      "|2025-01-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select to_date(\"2025-01-02\", \"yyyy-MM-dd\") as dt\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a3a128-8278-4434-9785-9bf6f29de091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         a|\n",
      "+----------+\n",
      "|2025-01-02|\n",
      "|2025-02-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dates = spark.sparkContext.parallelize([(\"2025-01-02\",),(\"2025-02-02\",)]).toDF([\"a\"])\n",
    "df_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07903090-8f69-485c-8f55-d99726ab7abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: date (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|         a|         b|\n",
      "+----------+----------+\n",
      "|2025-01-02|2025-01-02|\n",
      "|2025-02-02|2025-02-02|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dates_1 = df_dates.withColumn(\"b\", F.to_date(F.col(\"a\"), \"yyyy-MM-dd\"))\n",
    "df_dates_1.printSchema()\n",
    "df_dates_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450acfbc-a646-4ddd-ab08-d23e191be35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: date (nullable = true)\n",
      " |-- c: timestamp (nullable = true)\n",
      "\n",
      "+----------+----------+-------------------+\n",
      "|         a|         b|                  c|\n",
      "+----------+----------+-------------------+\n",
      "|2025-01-02|2025-01-02|2025-01-02 00:00:00|\n",
      "|2025-02-02|2025-02-02|2025-02-02 00:00:00|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dates_2 = df_dates_1.withColumn(\"c\", F.to_timestamp(F.col(\"a\"), \"yyyy-MM-dd\"))\n",
    "df_dates_2.printSchema()\n",
    "df_dates_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da3bb79-2790-43f2-b210-674090291a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: date (nullable = true)\n",
      " |-- c: timestamp (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: string (nullable = true)\n",
      "\n",
      "+----------+----------+-------------------+----------+----------+\n",
      "|         a|         b|                  c|         d|         e|\n",
      "+----------+----------+-------------------+----------+----------+\n",
      "|2025-01-02|2025-01-02|2025-01-02 00:00:00|2025-01-02|01-02-2025|\n",
      "|2025-02-02|2025-02-02|2025-02-02 00:00:00|2025-02-02|02-02-2025|\n",
      "+----------+----------+-------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dates_3 = (df_dates_2\n",
    "              .withColumn(\"d\", F.to_date(F.col(\"c\")))\n",
    "              .withColumn(\"e\", F.date_format(F.col(\"c\"), \"MM-dd-yyyy\")) \n",
    "             )\n",
    "df_dates_3.printSchema()\n",
    "df_dates_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c432c3-ee6f-440d-b8d1-0728086db3a4",
   "metadata": {},
   "source": [
    "# extracting a specific pattern from incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ed8ab9a-7066-4ae9-9d07-760f391f0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----------+\n",
      "|               value| rn|extracted 1|\n",
      "+--------------------+---+-----------+\n",
      "|At last, she foun...| 13|        the|\n",
      "|Disclaimer: While...| 14|        the|\n",
      "|Elara, though fri...| 15|        the|\n",
      "|Finally, Elara re...| 16|        the|\n",
      "|In the days of yo...| 17|        the|\n",
      "|Note: This is jus...| 18|        the|\n",
      "|One fateful morn,...| 19|        the|\n",
      "|The kingdom was s...| 22|        the|\n",
      "|With the sword in...| 23|        the|\n",
      "+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ext_1 = df_nodup.withColumn(\"extracted 1\", F.regexp_extract(\"value\", \"the\", 0)).where(F.col(\"extracted 1\") != \"\")\n",
    "df_ext_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e46fc2-0cbd-47c8-ae7f-6be05d7fc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----------+\n",
      "|               value| rn|extracted 1|\n",
      "+--------------------+---+-----------+\n",
      "|At last, she foun...| 13|        the|\n",
      "|Elara, though fri...| 15|        the|\n",
      "|In the days of yo...| 17|        the|\n",
      "|The kingdom was s...| 22|        the|\n",
      "|With the sword in...| 23|        the|\n",
      "+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ext_2 = df_ext_1.where(F.col(\"value\").contains(\"she\")) # startswith and endswith too\n",
    "df_ext_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cfcff-f6f2-4e92-866f-4b605af32644",
   "metadata": {},
   "source": [
    "# extracting from nested fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee7cedf-142e-45e8-a131-44bfbe513fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   a|\n",
      "+--------------------+\n",
      "|[Disclaimer:, Whi...|\n",
      "|[Prompt:, Generat...|\n",
      "|[A, Tale, of, Woe...|\n",
      "|[In, the, days, o...|\n",
      "|[One, fateful, mo...|\n",
      "|[Elara,, though, ...|\n",
      "|[With, the, sword...|\n",
      "|[Finally,, Elara,...|\n",
      "|[At, last,, she, ...|\n",
      "|[The, kingdom, wa...|\n",
      "|[Note:, This, is,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array = df_nodup.select(F.split(F.col(\"value\"),\" \").alias(\"a\")).where(F.size(F.col(\"a\"))>1)\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ed7b4-d371-4b88-b9a5-6cc5d5ad0a40",
   "metadata": {},
   "source": [
    "# explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adaaf2aa-c4e2-489d-aa4d-ed0e76f22ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                   a|          b|\n",
      "+--------------------+-----------+\n",
      "|[Disclaimer:, Whi...|Disclaimer:|\n",
      "|[Disclaimer:, Whi...|      While|\n",
      "|[Disclaimer:, Whi...|          I|\n",
      "|[Disclaimer:, Whi...|        can|\n",
      "|[Disclaimer:, Whi...|   generate|\n",
      "|[Disclaimer:, Whi...|       text|\n",
      "|[Disclaimer:, Whi...|         in|\n",
      "|[Disclaimer:, Whi...|          a|\n",
      "|[Disclaimer:, Whi...|      style|\n",
      "|[Disclaimer:, Whi...|reminiscent|\n",
      "|[Disclaimer:, Whi...|         of|\n",
      "|[Disclaimer:, Whi...|    classic|\n",
      "|[Disclaimer:, Whi...|    archaic|\n",
      "|[Disclaimer:, Whi...|   English,|\n",
      "|[Disclaimer:, Whi...|       it's|\n",
      "|[Disclaimer:, Whi...|  important|\n",
      "|[Disclaimer:, Whi...|         to|\n",
      "|[Disclaimer:, Whi...|       note|\n",
      "|[Disclaimer:, Whi...|       that|\n",
      "|[Disclaimer:, Whi...|       true|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array.withColumn(\"b\", F.explode(\"a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7925d-50fa-4a60-a121-dda94b37ffdf",
   "metadata": {},
   "source": [
    "# flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d35f2eb-d8c4-42d4-b1e9-9e22aa4b1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+------+\n",
      "|age|  id|        metadata|  name|\n",
      "+---+----+----------------+------+\n",
      "| 37|2111|[[a, 1], [b, 2]]|OLIVIA|\n",
      "+---+----+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array = [\"\"\"{\"id\": \"2111\", \"name\": \"OLIVIA\", \"age\": \"37\", \"metadata\":[[\"a\",1], [\"b\",2]]}\"\"\"]\n",
    "df_array = spark.sparkContext.parallelize(df_array)\n",
    "df_array = spark.read.json(df_array)\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03969fa5-5a27-4b84-958d-e8ae8a3233f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96625b9d-d29a-461f-bd9c-c694dec7481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+------+------------+\n",
      "|age|  id|        metadata|  name|           b|\n",
      "+---+----+----------------+------+------------+\n",
      "| 37|2111|[[a, 1], [b, 2]]|OLIVIA|[a, 1, b, 2]|\n",
      "+---+----+----------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array.withColumn(\"b\", F.flatten(\"metadata\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae967d0-9e09-4d66-b42e-9e572f803c78",
   "metadata": {},
   "source": [
    "# pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7642c679-5f11-49fe-9a0e-eb74942e328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+--------+\n",
      "| id|  name|value|pos| address|\n",
      "+---+------+-----+---+--------+\n",
      "|100|  John|   30|  1|Street 1|\n",
      "|200|Rodger| NULL|  1|Street 2|\n",
      "|300|   Tim|   80|  3|Street 3|\n",
      "|400|   Dan|   50|  4|Street 4|\n",
      "+---+------+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot = spark.sparkContext.parallelize( [(100, 'John', 30, 1, 'Street 1'),\n",
    "    (200, 'Rodger', None, 1, 'Street 2'),\n",
    "    (300, 'Tim', 80, 3, 'Street 3'),\n",
    "    (400, 'Dan', 50, 4, 'Street 4')] ).toDF((\"id\", \"name\", \"value\", \"pos\", \"address\"))\n",
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3872a522-f310-4051-bb23-3176285c34fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+\n",
      "|  name|   1|   3|   4|\n",
      "+------+----+----+----+\n",
      "|Rodger|NULL|NULL|NULL|\n",
      "|  John|  30|NULL|NULL|\n",
      "|   Tim|NULL|  80|NULL|\n",
      "|   Dan|NULL|NULL|  50|\n",
      "+------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_pivot\n",
    "     .groupBy(\"name\")\n",
    "     .pivot(\"pos\")\n",
    "     .agg(F.first(\"value\"))\n",
    "     .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db3db6-4574-4d53-8ab8-8eeaaec96c6b",
   "metadata": {},
   "source": [
    "SQL implementation is more restrictive than dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c75531d-5834-44f6-b29b-57ee63537639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----+----+\n",
      "| id|pos| address|john|mike|\n",
      "+---+---+--------+----+----+\n",
      "|300|  3|Street 3|NULL|  80|\n",
      "|100|  1|Street 1|  30|NULL|\n",
      "|400|  4|Street 4|NULL|NULL|\n",
      "|200|  1|Street 2|NULL|NULL|\n",
      "+---+---+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.createOrReplaceTempView(\"df_pivot\")\n",
    "sql = \"\"\"\n",
    "SELECT * FROM df_pivot\n",
    "    PIVOT (\n",
    "        FIRST(value) AS v\n",
    "        FOR name IN ('John' AS john, 'Tim' AS mike)\n",
    "    );\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb171da-5fd6-4233-beca-4f777d5e1f5c",
   "metadata": {},
   "source": [
    "# case ... when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd8605a3-edd5-4f79-9c8b-f1a23250afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|               value| rn|                   a|\n",
      "+--------------------+---+--------------------+\n",
      "|                    |  1|                    |\n",
      "|                    |  2|                    |\n",
      "|                    |  3|                   !|\n",
      "|                    |  4|                    |\n",
      "|                    |  5|                    |\n",
      "|                    |  6|                   !|\n",
      "|                    |  7|                    |\n",
      "|                    |  8|                    |\n",
      "|                    |  9|                   !|\n",
      "|                    | 10|                    |\n",
      "|                    | 11|                    |\n",
      "|A Tale of Woe and...| 12|!A Tale of Woe an...|\n",
      "|At last, she foun...| 13|At last, she foun...|\n",
      "|Disclaimer: While...| 14|Disclaimer: While...|\n",
      "|Elara, though fri...| 15|!Elara, though fr...|\n",
      "|Finally, Elara re...| 16|Finally, Elara re...|\n",
      "|In the days of yo...| 17|In the days of yo...|\n",
      "|Note: This is jus...| 18|!Note: This is ju...|\n",
      "|One fateful morn,...| 19|One fateful morn,...|\n",
      "|Prompt: Generate ...| 20|Prompt: Generate ...|\n",
      "+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nodup.withColumn(\"a\",\n",
    "    F.when(F.col(\"rn\")%3 == 0, F.concat(F.lit(\"!\"), F.col(\"value\")))\n",
    "    .otherwise(F.col(\"value\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012b147-6dc1-47e5-b48f-db07d62fb439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479e91e-8953-4606-8ecb-af3617d401e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833ed2c-3811-4268-93c8-bc43328f4694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43a729-9830-4106-b068-660d43bbd8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e27e8-5500-43cc-bda1-5736b1dc7566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e6dc3-87a0-42de-a6aa-e8ba70f7ca43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bc965f-671e-4c85-9cbb-af6a1d1f4fc4",
   "metadata": {},
   "source": [
    "Read in a single file and make duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14cd853-a6a6-4c44-9b82-7065b4d3a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sparkuser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sparkuser/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ca5819cc-43ab-4942-8efb-c782527bd356;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.1.0/delta-spark_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.1.0!delta-spark_2.12.jar (2890ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.1.0/delta-storage-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.1.0!delta-storage.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (85ms)\n",
      ":: resolution report :: resolve 5662ms :: artifacts dl 3014ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ca5819cc-43ab-4942-8efb-c782527bd356\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (5727kB/8ms)\n",
      "25/02/13 21:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta import *\n",
    "from delta.tables import DeltaTable\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000) # Adjust as needed\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\").master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    ".getOrCreate())\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7483273-74be-442b-ae51-8864b25d455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'spark': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6299cec0-3d68-4b23-b21e-71ac044ef258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b99a7c-b425-4a28-a4ec-2d9cdd8279cf",
   "metadata": {},
   "source": [
    "make several iterations of changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d35f2eb-d8c4-42d4-b1e9-9e22aa4b1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+------+\n",
      "|age|  id|        metadata|  name|\n",
      "+---+----+----------------+------+\n",
      "| 37|2111|[[a, 1], [b, 2]]|OLIVIA|\n",
      "+---+----+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array = [\"\"\"{\"id\": \"2111\", \"name\": \"OLIVIA\", \"age\": \"37\", \"metadata\":[[\"a\",1], [\"b\",2]]}\"\"\"]\n",
    "df_array = spark.sparkContext.parallelize(df_array)\n",
    "df_array = spark.read.json(df_array)\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03969fa5-5a27-4b84-958d-e8ae8a3233f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_array.write.format(\"delta\").mode(\"overwrite\").save(\"spark\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25899a9-5ce4-4115-a695-f203608bfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array_2 = df_array.withColumn(\"name\", F.lower(F.col(\"name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b750e1d5-0278-4118-ad7c-24796099f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array_2.write.format(\"delta\").mode(\"overwrite\").save(\"spark\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339d6b03-0bac-4c93-a9a9-e91bd1fac29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array_3 = df_array_2.withColumn(\"name\", F.concat(F.lit(\"abab \"), F.col(\"name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f0f48f-5193-4390-9d83-f333fc9485e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array_3.write.format(\"delta\").mode(\"overwrite\").save(\"spark\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73d75ae-43f0-4767-b609-86a8d9b1d8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>2111</td>\n",
       "      <td>[[a, 1], [b, 2]]</td>\n",
       "      <td>abab olivia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age    id          metadata         name\n",
       "0  37  2111  [[a, 1], [b, 2]]  abab olivia"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"spark\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fcc49-c70e-44ed-8301-25215b391def",
   "metadata": {},
   "source": [
    "know how files are layed out in the delta lakes format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f012b147-6dc1-47e5-b48f-db07d62fb439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_delta_log\n",
      "part-00000-0aac5455-055c-4fd8-8eff-4b3c478fe554-c000.snappy.parquet\n",
      "part-00000-253893b7-4e39-491d-a386-0338f7575ce3-c000.snappy.parquet\n",
      "part-00000-872ffadb-d922-4407-a6e9-c722cc6eab24-c000.snappy.parquet\n",
      "part-00007-51640031-68de-4db2-a3a4-d16dfe2598b2-c000.snappy.parquet\n",
      "part-00007-6fcf38e4-466f-48d0-853f-c22a1d3dd9da-c000.snappy.parquet\n",
      "part-00007-a2d140ac-0137-457e-8cb7-8c96eb1d0b68-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b833ed2c-3811-4268-93c8-bc43328f4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json  00000000000000000001.json  00000000000000000002.json\n"
     ]
    }
   ],
   "source": [
    "!ls spark/_delta_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c06b8-22bf-4801-a5c1-a92f2d330100",
   "metadata": {},
   "source": [
    "look at the tables history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814e27e8-5500-43cc-bda1-5736b1dc7566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-02-13 21:21:30.272</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2111', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-02-13 21:21:26.373</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 21:21:21.291</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation                         operationParameters   job notebook clusterId  readVersion isolationLevel  isBlindAppend                                                   operationMetrics userMetadata                           engineInfo\n",
       "0        2 2025-02-13 21:21:30.272   None     None     WRITE  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          1.0   Serializable          False  {'numOutputRows': '1', 'numOutputBytes': '2111', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "1        1 2025-02-13 21:21:26.373   None     None     WRITE  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          0.0   Serializable          False  {'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "2        0 2025-02-13 21:21:21.291   None     None     WRITE  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          NaN   Serializable          False  {'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"spark\")\n",
    "history_df = delta_table.history()\n",
    "history_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80603937-1a07-40ec-870e-3bec28bb6dc9",
   "metadata": {},
   "source": [
    "restoring a table to a previous version based on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0549196a-e97b-444e-9a4c-edcebfdf16ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.restoreToVersion(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ca35a0-7c76-4c24-9b5a-e201a3bc9a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-13 21:21:45.673</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>RESTORE</td>\n",
       "      <td>{'version': '1', 'timestamp': None}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'removedFilesSize': '1390', 'tableSizeAfterRestore': '1355', 'numRemovedFiles': '1', 'restoredFilesSize': '1355', 'numOfFilesAfterRestore': '1', 'numRestoredFiles': '1'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-02-13 21:21:30.272</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2111', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-02-13 21:21:26.373</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 21:21:21.291</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation                         operationParameters   job notebook clusterId  readVersion isolationLevel  isBlindAppend                                                                                                                                                            operationMetrics userMetadata                           engineInfo\n",
       "0        3 2025-02-13 21:21:45.673   None     None   RESTORE         {'version': '1', 'timestamp': None}  None     None      None          2.0   Serializable          False  {'removedFilesSize': '1390', 'tableSizeAfterRestore': '1355', 'numRemovedFiles': '1', 'restoredFilesSize': '1355', 'numOfFilesAfterRestore': '1', 'numRestoredFiles': '1'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "1        2 2025-02-13 21:21:30.272   None     None     WRITE  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          1.0   Serializable          False                                                                                                           {'numOutputRows': '1', 'numOutputBytes': '2111', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "2        1 2025-02-13 21:21:26.373   None     None     WRITE  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          0.0   Serializable          False                                                                                                           {'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "3        0 2025-02-13 21:21:21.291   None     None     WRITE  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          NaN   Serializable          False                                                                                                           {'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"spark\")\n",
    "history_df = delta_table.history()\n",
    "history_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145e09f8-170b-45c2-a5ae-8f358ebbb31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-02-13 21:21:26'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_we_want_to_rollback_to = delta_table.history().where(F.col(\"version\") == 1).select(\"timestamp\").collect()[0]['timestamp'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "timestamp_we_want_to_rollback_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5882050-fcf4-4880-aba4-7685e6cc76c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.restoreToTimestamp(timestamp_we_want_to_rollback_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a499ef-880a-4256-8112-1aa52b478e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-02-13 21:21:48.427</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>RESTORE</td>\n",
       "      <td>{'version': None, 'timestamp': '2025-02-13 21:21:26.0'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'removedFilesSize': '1355', 'tableSizeAfterRestore': '1355', 'numRemovedFiles': '1', 'restoredFilesSize': '1355', 'numOfFilesAfterRestore': '1', 'numRestoredFiles': '1'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-13 21:21:45.673</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>RESTORE</td>\n",
       "      <td>{'version': '1', 'timestamp': None}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'removedFilesSize': '1390', 'tableSizeAfterRestore': '1355', 'numRemovedFiles': '1', 'restoredFilesSize': '1355', 'numOfFilesAfterRestore': '1', 'numRestoredFiles': '1'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-02-13 21:21:30.272</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2111', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-02-13 21:21:26.373</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 21:21:21.291</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.4 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation                                      operationParameters   job notebook clusterId  readVersion isolationLevel  isBlindAppend                                                                                                                                                            operationMetrics userMetadata                           engineInfo\n",
       "0        4 2025-02-13 21:21:48.427   None     None   RESTORE  {'version': None, 'timestamp': '2025-02-13 21:21:26.0'}  None     None      None          3.0   Serializable          False  {'removedFilesSize': '1355', 'tableSizeAfterRestore': '1355', 'numRemovedFiles': '1', 'restoredFilesSize': '1355', 'numOfFilesAfterRestore': '1', 'numRestoredFiles': '1'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "1        3 2025-02-13 21:21:45.673   None     None   RESTORE                      {'version': '1', 'timestamp': None}  None     None      None          2.0   Serializable          False  {'removedFilesSize': '1390', 'tableSizeAfterRestore': '1355', 'numRemovedFiles': '1', 'restoredFilesSize': '1355', 'numOfFilesAfterRestore': '1', 'numRestoredFiles': '1'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "2        2 2025-02-13 21:21:30.272   None     None     WRITE               {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          1.0   Serializable          False                                                                                                           {'numOutputRows': '1', 'numOutputBytes': '2111', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "3        1 2025-02-13 21:21:26.373   None     None     WRITE               {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          0.0   Serializable          False                                                                                                           {'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0\n",
       "4        0 2025-02-13 21:21:21.291   None     None     WRITE               {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None          NaN   Serializable          False                                                                                                           {'numOutputRows': '1', 'numOutputBytes': '2076', 'numFiles': '2'}         None  Apache-Spark/3.5.4 Delta-Lake/3.1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"spark\")\n",
    "history_df = delta_table.history()\n",
    "history_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea054e55-8d00-488c-83db-bf6d61d8a83d",
   "metadata": {},
   "source": [
    "query the based off of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cff35c5c-f312-4176-9af4-a00b0202ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>2111</td>\n",
       "      <td>[[a, 1], [b, 2]]</td>\n",
       "      <td>OLIVIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age    id          metadata    name\n",
       "0  37  2111  [[a, 1], [b, 2]]  OLIVIA"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_we_want_to_rollback_to).load(\"spark\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78dfd71c-a7df-4514-aecf-5e7a45bdfbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>2111</td>\n",
       "      <td>[[a, 1], [b, 2]]</td>\n",
       "      <td>olivia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age    id          metadata    name\n",
       "0  37  2111  [[a, 1], [b, 2]]  olivia"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"spark\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f8a9e-4664-4858-b55c-1a17be3cd317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
